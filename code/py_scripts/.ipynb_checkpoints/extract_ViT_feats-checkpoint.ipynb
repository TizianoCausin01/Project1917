{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50287736-d59f-4754-b8bb-f45601bc7f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutorial found at https://huggingface.co/docs/transformers/en/model_doc/vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1529070-11a7-413f-b8a3-a395af55f015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoImageProcessor, ViTConfig, ViTModel, ViTForImageClassification\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "pipeline = pipeline(\n",
    "    task=\"image-classification\",\n",
    "    model=\"google/vit-base-patch16-224\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab2079d4-11c6-4fea-b0bc-2606b0d20d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a ViT vit-base-patch16-224 style configuration\n",
    "configuration = ViTConfig()\n",
    "\n",
    "# Initializing a model (with random weights) from the vit-base-patch16-224 style configuration\n",
    "model = ViTModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44d5a6df-50c8-480a-8db6-912b9b7aa472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Egyptian cat\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\", trust_remote_code=True)\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\") # resizes and crops the img just picking up the center # FIXME custom the parameters of cropping and of normalization\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "inputs = image_processor(image, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_label = logits.argmax(-1).item()\n",
    "print(model.config.id2label[predicted_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4efeb879-befd-4569-b676-a69f44b2a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTForImageClassification(\n",
      "  (vit): ViTModel(\n",
      "    (embeddings): ViTEmbeddings(\n",
      "      (patch_embeddings): ViTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): ViTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x ViTLayer(\n",
      "          (attention): ViTAttention(\n",
      "            (attention): ViTSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (output): ViTSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ViTIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ViTOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "348d21ef-9800-4d32-a71a-e77546c69650",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_patches = 14**2\n",
    "layer_dim = 768\n",
    "out_dim = (num_patches+1) *layer_dim\n",
    "encoder_blocks = 12\n",
    "rand_idx = []\n",
    "for block in range(encoder_blocks):\n",
    "    rand_idx.append(\n",
    "        np.random.choice(np.arange(out_dim), size=out_dim // 100, replace=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d46d7af-f580-4f82-89ef-16a6e59c7b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_hook(layer, rand_idx): # FIXME add randidx\n",
    "    def hook_func(module, input, output):\n",
    "        out = output.detach().half().reshape(-1)\n",
    "        out = out[rand_idx]\n",
    "        feats[layer].append(\n",
    "            out\n",
    "        )  # half makes it become float16, reshape(-1) vectorizes it\n",
    "\n",
    "    return hook_func\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a67b1920-de5a-4314-924c-b936a6b40a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTOutput(\n",
      "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# FIXME add the hooks appropriately\n",
    "print(model.vit.encoder.layer[1].output) # or .encoder .classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "282c19d1-ba27-4a9d-8c59-f3dbcc030d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.vit.encoder.layer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f1dcfae-8320-49cf-b3bd-9baa5256f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_handle = []\n",
    "# here we are hooking the output of of the 12 transformers blocks \n",
    "# they all end with an MLP 768>3072>768 -> importantly, it processes each of the patches indepently and identically\n",
    "# so the output will be (batch_size, num_tokens, hidden_dim). \n",
    "# Leaving aside the batch_size, the tokens (embeddings for the patches) are on the rows, they are (224^2 / 16^2) +1 = 196+1 (the +1 is given by the classification token, a summary of the img)\n",
    "for block_idx in range(encoder_blocks):\n",
    "    hook_handle.append(\n",
    "        model.vit.encoder.layer[block_idx].output.register_forward_hook(\n",
    "                wrapper_hook(block_idx, rand_idx[block_idx])\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a59f43de-e26f-4af8-87ff-99d72f4a0560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for h in hook_handle:\n",
    "#    h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "823431ab-9268-4bf9-b77c-ab6f1916e23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Egyptian cat\n"
     ]
    }
   ],
   "source": [
    "feats = {i: [] for i in range(encoder_blocks)}\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_label = logits.argmax(-1).item()\n",
    "print(model.config.id2label[predicted_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "249bb9f9-9935-452b-941b-dcb6743319c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor.size(feats[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "776ef125-98eb-4dd8-8df6-1d4724080901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2914,   6080, 119734, ...,  45219,  27286, 144591])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_idx[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12be3cd7-f0fe-4adf-80ac-cdcda50f0b80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1917_py_env",
   "language": "python",
   "name": "1917_py_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
