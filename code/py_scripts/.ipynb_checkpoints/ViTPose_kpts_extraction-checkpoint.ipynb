{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3b72edc9-f1b7-4955-b7dd-fe4427e9dc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutorial from https://huggingface.co/docs/transformers/en/model_doc/vitpose\n",
    "from transformers import AutoProcessor, RTDetrForObjectDetection, VitPoseForPoseEstimation\n",
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "45f37e44-8fac-4bd2-bc3b-ab9048b59086",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = {\n",
    "    \"kpts\" : [],\n",
    "    \"boxes\" : [], \n",
    "    \"score_boxes\" : [],\n",
    "    \"score_kpts\" : [], \n",
    "    \"head_kpts\" : [],\n",
    "    \"score_heads\" : []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c91b46-4a63-4a54-bdac-067e625c8022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_res(res: list, key: str, size: tuple, top_k: int, box=0): \n",
    "    if box == 0:\n",
    "        data = [res[i][key].cpu().numpy() for i in range(len(res))]\n",
    "    else:\n",
    "        data = [np.array([i.cpu().numpy()]) for i in res[\"scores\"]]\n",
    "    if len(data) < top_k: # fills in if there are less than 5 pers\n",
    "        fill_in = [np.full(size, np.nan) for _ in range(int(top_k - len(data)))]\n",
    "        data.extend(fill_in)\n",
    "    \n",
    "    data = data[0:5]\n",
    "    data = np.stack(data, axis=-1) # stacks them along the last dimension\n",
    "    data = data.flatten(order='F') # vectorizes it fortran style (column-major like matlab)\n",
    "    return data \n",
    "# EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9830bfc3-b78d-46cc-b2a0-acb71c929da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - load video\n",
    "path2mod = \"/Volumes/TIZIANO/models\"\n",
    "\n",
    "# 2 - load models\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "person_image_processor = AutoProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\") # loads a preprocessing pipeline img (to ensure same preproc.g)\n",
    "person_model = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\", device_map=device) # loads the object detection model:  RT-DETR object detection model (detectiion + label.g -> label 0 = person)\n",
    "#inputs = person_image_processor(images=image, return_tensors=\"pt\").to(device) # preprocesses the image and returns it as a tensor\n",
    "# returns an object of type <class 'transformers.image_processing_base.BatchFeature'> -> behaves like a dict e.g. inputs.keys() -> 'pixel_values' i.e. the normalized img tensor of shape 2, 3, 256, 192\n",
    "image_processor = AutoProcessor.from_pretrained(\"usyd-community/vitpose-base-simple\") # downloads a processor tailored for the vitpose-base-simple model, it resizes, normalizes, and formats input data (cropping each detected person), automatically includes COCO keypoint configuration.\n",
    "model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-base-simple\", device_map=device) #downloads ViTPose\n",
    "\n",
    "# 3 - read frame and preprocess it\n",
    "runs = [1,2,3]\n",
    "for irun in runs  \n",
    "    path2vid = f\"/Volumes/TIZIANO/stimuli/Project1917_movie_part{irun}_24Hz.mp4\"\n",
    "    reader = cv2.VideoCapture(path2vid)\n",
    "    while True:\n",
    "        ret, frame = reader.read()\n",
    "        \n",
    "        if ret == False:\n",
    "            break\n",
    "        # end if ret==False:\n",
    "    \n",
    "        frame_rgb = cv2.cvtColor(\n",
    "            frame, cv2.COLOR_BGR2RGB\n",
    "        )  # converts to bgr to rgb color codes\n",
    "    \n",
    "        inputs = person_image_processor(frame_rgb, return_tensors=\"pt\")\n",
    "        \n",
    "    # 4 - detect people\n",
    "        with torch.no_grad():\n",
    "            outputs = person_model(**inputs) # performs object detection on the input\n",
    "    \n",
    "    # 5 - get box predictions\n",
    "        result = person_image_processor.post_process_object_detection(\n",
    "            outputs, target_sizes=torch.tensor([(frame_rgb.shape[0], frame_rgb.shape[1])]), threshold=0.3 # converts raw model outputs into interpretable bounding box predictions \n",
    "        )[0] # selects the first element in the list bc only one img\n",
    "        \n",
    "        person_boxes = result[\"boxes\"][result[\"labels\"] == 0] # index only the boxes associated with label 0 (person) in COCO class labels\n",
    "        \n",
    "        score_boxes = result[\"scores\"][result[\"labels\"] == 0]\n",
    "        score_boxes = score_boxes.cpu().numpy()\n",
    "        # score_boxes_store = fill_in_res(person_boxes, \"scores\", (1, 1), 5) \n",
    "        # feats[\"score_boxes\"].append(score_boxes_store)\n",
    "        # converts boxes from VOC format: (x1, y1, x2, y2) to COCO format: N pers detected x 4 -> 4 cols are => (x, y, width, height)\n",
    "        person_boxes[:, 2] = person_boxes[:, 2] - person_boxes[:, 0] \n",
    "        person_boxes[:, 3] = person_boxes[:, 3] - person_boxes[:, 1] \n",
    "        \n",
    "    # 6 - preprocess for kpt detection\n",
    "        inputs = image_processor(frame_rgb, boxes=[person_boxes], return_tensors=\"pt\").to(device) # processes the original image using the bounding boxes -> ViTPose expects tightly cropped pics\n",
    "        # inputs is a dict like type with \"pixels_value\" as only entry. It is a tensor [Batch, Channels, Height, Width] -> Batch is the number of people detected\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs) # runs ViTPose\n",
    "        pose_results = image_processor.post_process_pose_estimation(outputs, boxes=[person_boxes])[0]\n",
    "        kpts_store = fill_in_res(pose_results, \"keypoints\", (17,2), 5)\n",
    "        kpts_scores_store = fill_in_res(pose_results, \"scores\", (17), 5)\n",
    "        person_boxes_store = fill_in_res(pose_results, \"bbox\", (4), 5) \n",
    "        score_boxes_store = fill_in_res(result, \"scores\", (1), 5, box=1)\n",
    "        feats[\"boxes\"].append(person_boxes_store) # FIXME it's a list of dicts\n",
    "        feats[\"kpts\"].append(kpts_store)\n",
    "        feats[\"score_kpts\"].append(kpts_scores_store)\n",
    "        feats[\"score_boxes\"].append(score_boxes_store)\n",
    "        with h5py.File(f\"{path2mod}/Project1917_ViTPose_run0{irun}.h5\", \"w\") as f:\n",
    "            # Iterate over dictionary items and save them in the HDF5 file\n",
    "            for key, value in feats.items():\n",
    "                f.create_dataset(key, data=value)  # Create a dataset for each key-value pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df3b9493-e561-4981-af13-911caa95f225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L_Ankle': 15,\n",
       " 'L_Ear': 3,\n",
       " 'L_Elbow': 7,\n",
       " 'L_Eye': 1,\n",
       " 'L_Hip': 11,\n",
       " 'L_Knee': 13,\n",
       " 'L_Shoulder': 5,\n",
       " 'L_Wrist': 9,\n",
       " 'Nose': 0,\n",
       " 'R_Ankle': 16,\n",
       " 'R_Ear': 4,\n",
       " 'R_Elbow': 8,\n",
       " 'R_Eye': 2,\n",
       " 'R_Hip': 12,\n",
       " 'R_Knee': 14,\n",
       " 'R_Shoulder': 6,\n",
       " 'R_Wrist': 10}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.label2id "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1917_py_env",
   "language": "python",
   "name": "1917_py_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
